{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GiuU7RrO-JkL"
      },
      "source": [
        "# Model training with CREDO image dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9LHbRMGbDNj"
      },
      "outputs": [],
      "source": [
        "%run ./notebook_init.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms\n",
        "from sklearn import metrics\n",
        "\n",
        "from core import DATA_FOLDER\n",
        "from scripts.credo_training_utils import TRAINING_FOLDERPATH,\\\n",
        "    ModelTraining, ImageFolderWithPath, Seed,\\\n",
        "    resnet18_model, predict_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYJRXuPCqMdP"
      },
      "outputs": [],
      "source": [
        "processed_data_folder = os.path.join(DATA_FOLDER, \"credo_processed_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = Seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_size = (60,60)\n",
        "\n",
        "data_transforms = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.RandomRotation((0, 360), fill=(0,)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(0, 1)\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.RandomRotation((0, 360), fill=(0,)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(0, 1)\n",
        "    ]),\n",
        "    \"test\": transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(0, 1)\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folders_list = [\"train\", \"val\", \"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_datasets = {x: ImageFolderWithPath(os.path.join(processed_data_folder, x),\n",
        "                                         data_transforms[x])\n",
        "                                         for x in folders_list}\n",
        "\n",
        "dataloaders = {\n",
        "    x: torch.utils.data.DataLoader(\n",
        "        image_datasets[x],\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        worker_init_fn=seed.seed_worker\n",
        "    ) for x in folders_list\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_sizes = {x: len(image_datasets[x]) for x in folders_list}\n",
        "class_names = image_datasets[\"train\"].classes\n",
        "dataset_class_qty = {x: dict(Counter(image_datasets[x].targets)) for x in folders_list}\n",
        "class_qty = len(class_names)\n",
        "\n",
        "print(f\"Class quantity: {class_qty}\")\n",
        "print(f\"Class names: {class_names}\")\n",
        "for i in folders_list: print(f\"{i}: {dataset_class_qty[i]}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imshow(input_img, title=None):\n",
        "    \"\"\" Imshow for Tensor \"\"\"\n",
        "    img = np.asarray(input_img).transpose((1, 2, 0))\n",
        "    plt.imshow(img, vmin=0, vmax=5)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    # pause a bit to update plots\n",
        "    plt.pause(0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Batch of training data\")\n",
        "# Iterate through the data loader\n",
        "inputs, classes, _ = next(iter(dataloaders[\"train\"]))\n",
        "# Generate image grid\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "print(\"Batch of validation data\")\n",
        "inputs, classes, _ = next(iter(dataloaders[\"val\"]))\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "print(\"Batch of test data\")\n",
        "inputs, classes, _ = next(iter(dataloaders[\"test\"]))\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_data_folder = os.path.join(TRAINING_FOLDERPATH, \"best_model_weight\")\n",
        "os.makedirs(model_data_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_filepath = os.path.join(model_data_folder, \"best_model_params.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet18 = resnet18_model(device, class_qty)\n",
        "print(resnet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(resnet18, (3, 64, 64))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 150\n",
        "\n",
        "model_training = ModelTraining(resnet18)\n",
        "model_ft_randstart = model_training.train_model(device, dataloaders,\n",
        "                                                dataset_sizes,\n",
        "                                                num_epochs,\n",
        "                                                best_model_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_train, acc_val, loss_train, loss_val = model_training.get_acc_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(loss_train, label=\"Train\")\n",
        "plt.plot(loss_val,label=\"Validation\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend()\n",
        "plt.xlim(0, num_epochs)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(acc_train, label=\"Train\")\n",
        "plt.plot(acc_val, label=\"Validation\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend()\n",
        "plt.xlim(0, num_epochs)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysing the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_folder = os.path.join(TRAINING_FOLDERPATH, \"metrics\")\n",
        "os.makedirs(metrics_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saved_model = resnet18_model(device, class_qty)\n",
        "saved_model.load_state_dict(torch.load(best_model_filepath))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_label, true_label = predict_model(device, saved_model, class_names, dataloaders[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_accuracy = metrics.accuracy_score(true_label, predicted_label)\n",
        "test_precision = metrics.precision_score(true_label, predicted_label, average=\"macro\")\n",
        "test_recall = metrics.recall_score(true_label, predicted_label, average=\"macro\")\n",
        "test_bal_accuracy = metrics.balanced_accuracy_score(true_label, predicted_label)\n",
        "test_f1 = metrics.f1_score(true_label, predicted_label, average=\"macro\")\n",
        "\n",
        "print(\"Test Accuracy: {:.4f}\".format(test_accuracy))\n",
        "print(\"Test Precision: {:.4f}\".format(test_precision))\n",
        "print(\"Test Recall: {:.4f}\".format(test_recall))\n",
        "print(\"Test Balanced Accuracy: {:.4f}\".format(test_bal_accuracy))\n",
        "print(\"Test F1-Score: {:.4f}\".format(test_f1))\n",
        "\n",
        "print(\"\\nConfusion Matrix - Test data\")\n",
        "confusion_mtx = metrics.confusion_matrix(true_label, predicted_label)\n",
        "print(confusion_mtx)\n",
        "\n",
        "with open(os.path.join(metrics_folder, \"metrics.txt\"), \"w\") as metrics_txt:\n",
        "    metrics_txt.write(f\"Test Accuracy\\t {test_accuracy:.4f}\\n\")\n",
        "    metrics_txt.write(f\"Test Precision\\t {test_precision:.4f}\\n\")\n",
        "    metrics_txt.write(f\"Test Recall\\t {test_recall:.4f}\\n\")\n",
        "    metrics_txt.write(f\"Test Balanced Accuracy\\t {test_bal_accuracy:.4f}\\n\")\n",
        "    metrics_txt.write(f\"Test F1-Score:\\t {test_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics.ConfusionMatrixDisplay(confusion_mtx, display_labels=class_names).plot()\n",
        "plt.title(\"Confusion Matrix - Test data\")\n",
        "plt.grid(False)\n",
        "plt.savefig(os.path.join(metrics_folder, \"confusion_mtx.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classification_report = metrics.classification_report(true_label, predicted_label,\n",
        "                                                      zero_division=1, output_dict=True,\n",
        "                                                      target_names=class_names)\n",
        "sns.heatmap(pd.DataFrame(classification_report).iloc[:-1, :].T, annot=True)\n",
        "plt.title(\"Classification report - Test data\")\n",
        "plt.savefig(os.path.join(metrics_folder, \"classification_report.png\"), bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TnQJJiRoGtYT"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
